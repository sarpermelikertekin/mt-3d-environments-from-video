{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories and model path\n",
    "model_path = ''\n",
    "image_dir = r\"\"\n",
    "ground_truth_dir = r\"\"\n",
    "save_dir = r\"\"\n",
    "dataset = \"\"\n",
    "# Use for better visualization and selecting indices to be showed\n",
    "starting_index = 0\n",
    "max_files=8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidian Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def compute_euclidean_distance(row1, row2):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance for:\n",
    "    - Bounding box center (x, y)\n",
    "    - Bounding box size (x, y)\n",
    "    - Corners (8 pairs)\n",
    "    \"\"\"\n",
    "    distances = {\n",
    "        \"center\": euclidean(row1[1:3], row2[1:3]),\n",
    "        \"size\": euclidean(row1[3:5], row2[3:5]),\n",
    "        \"corners\": [euclidean(row1[5 + i * 2:7 + i * 2], row2[5 + i * 2:7 + i * 2])\n",
    "                    for i in range(8)]\n",
    "    }\n",
    "    return distances\n",
    "\n",
    "\n",
    "def find_closest_match(predictions, ground_truth, width, height):\n",
    "    \"\"\"\n",
    "    Finds the closest match for each prediction in the ground truth based on Euclidean distance.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    if predictions.empty or ground_truth.empty:\n",
    "        print(\"One of the DataFrames is empty, skipping...\")\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        # Group ground truth and predictions by object ID\n",
    "        gt_groups = ground_truth.groupby(0)\n",
    "        pred_groups = predictions.groupby(0)\n",
    "\n",
    "        # Get the intersection of IDs present in both predictions and ground truth\n",
    "        common_ids = set(gt_groups.groups.keys()) & set(pred_groups.groups.keys())\n",
    "\n",
    "        for obj_id in common_ids:\n",
    "            gt_rows = ground_truth.loc[gt_groups.groups[obj_id]].copy()\n",
    "            pred_rows = predictions.loc[pred_groups.groups[obj_id]].copy()\n",
    "\n",
    "            # Match predictions and ground truth for this object ID\n",
    "            while not gt_rows.empty and not pred_rows.empty:\n",
    "                min_distances = None\n",
    "                min_gt_index = None\n",
    "                min_pred_index = None\n",
    "\n",
    "                # Find the closest ground truth-prediction pair\n",
    "                for gt_index, gt_row in gt_rows.iterrows():\n",
    "                    for pred_index, pred_row in pred_rows.iterrows():\n",
    "                        distances = compute_euclidean_distance(pred_row, gt_row)\n",
    "                        if min_distances is None or distances[\"center\"] < min_distances[\"center\"]:\n",
    "                            min_distances = distances\n",
    "                            min_gt_index = gt_index\n",
    "                            min_pred_index = pred_index\n",
    "\n",
    "                # Multiply normalized values by width and height to get non-normalized distances\n",
    "                pred_row_non_normalized = pred_rows.loc[min_pred_index].copy()\n",
    "                pred_row_non_normalized[1] *= width\n",
    "                pred_row_non_normalized[2] *= height\n",
    "                pred_row_non_normalized[3] *= width\n",
    "                pred_row_non_normalized[4] *= height\n",
    "                for i in range(8):\n",
    "                    pred_row_non_normalized[5 + i * 2] *= width\n",
    "                    pred_row_non_normalized[6 + i * 2] *= height\n",
    "\n",
    "                gt_row_non_normalized = gt_rows.loc[min_gt_index].copy()\n",
    "                gt_row_non_normalized[1] *= width\n",
    "                gt_row_non_normalized[2] *= height\n",
    "                gt_row_non_normalized[3] *= width\n",
    "                gt_row_non_normalized[4] *= height\n",
    "                for i in range(8):\n",
    "                    gt_row_non_normalized[5 + i * 2] *= width\n",
    "                    gt_row_non_normalized[6 + i * 2] *= height\n",
    "\n",
    "                # Compute non-normalized distances\n",
    "                non_normalized_distances = compute_euclidean_distance(pred_row_non_normalized, gt_row_non_normalized)\n",
    "\n",
    "                # Save the closest match\n",
    "                results.append({\n",
    "                    \"id\": obj_id,\n",
    "                    \"min_center_distance_normalized\": min_distances[\"center\"],\n",
    "                    \"min_size_distance_normalized\": min_distances[\"size\"],\n",
    "                    \"min_corner_distances_normalized\": min_distances[\"corners\"],\n",
    "                    \"min_center_distance_non_normalized\": non_normalized_distances[\"center\"],\n",
    "                    \"min_size_distance_non_normalized\": non_normalized_distances[\"size\"],\n",
    "                    \"min_corner_distances_non_normalized\": non_normalized_distances[\"corners\"]\n",
    "                })\n",
    "\n",
    "                # Remove matched rows\n",
    "                gt_rows.drop(index=min_gt_index, inplace=True)\n",
    "                pred_rows.drop(index=min_pred_index, inplace=True)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError encountered: {e}. Skipping problematic data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}. Skipping problematic data.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_file(model, image_path, gt_file):\n",
    "    \"\"\"\n",
    "    Process a single image and compare its predictions with ground truth.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform YOLO inference\n",
    "        results = model(image_path)[0]\n",
    "\n",
    "        # Get image dimensions\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "\n",
    "        # Prepare prediction data\n",
    "        predictions = []\n",
    "        for result in results:\n",
    "            # Object ID\n",
    "            obj_id = int(result.boxes.cls[0])  # Class ID\n",
    "\n",
    "            # Bounding box center and size\n",
    "            box = result.boxes.xyxy[0].tolist()\n",
    "            center_x = (box[0] + box[2]) / 2 / width\n",
    "            center_y = (box[1] + box[3]) / 2 / height\n",
    "            size_x = (box[2] - box[0]) / width\n",
    "            size_y = (box[3] - box[1]) / height\n",
    "\n",
    "            # Keypoints (if available)\n",
    "            keypoints = []\n",
    "            normalized_keypoints = []\n",
    "            if result.keypoints is not None:\n",
    "                keypoints = result.keypoints.xy[0].tolist()\n",
    "                keypoints = [(kp[0], kp[1]) for kp in keypoints[:8]]\n",
    "                normalized_keypoints = [(kp[0] / width, kp[1] / height) for kp in keypoints]\n",
    "\n",
    "            # Prepare prediction row\n",
    "            row = [obj_id, center_x, center_y, size_x, size_y]\n",
    "            row.extend(sum(normalized_keypoints, ()))\n",
    "            predictions.append(row)\n",
    "\n",
    "        # Convert predictions to DataFrame\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "        # Load ground truth\n",
    "        ground_truth = pd.read_csv(gt_file, header=None)\n",
    "\n",
    "        # Ensure DataFrames are not empty\n",
    "        if predictions_df.empty or ground_truth.empty:\n",
    "            print(f\"Empty DataFrame encountered for {os.path.basename(image_path)}, skipping...\")\n",
    "            return {\"file\": os.path.basename(image_path), \"objects\": []}\n",
    "\n",
    "        # Find closest matches\n",
    "        results = find_closest_match(predictions_df, ground_truth, width, height)\n",
    "\n",
    "        # Prepare results for the file\n",
    "        file_results = {\"file\": os.path.basename(image_path), \"objects\": []}\n",
    "        for result in results:\n",
    "            avg_corner_error_normalized = np.mean(result[\"min_corner_distances_normalized\"])\n",
    "            avg_corner_error_non_normalized = np.mean(result[\"min_corner_distances_non_normalized\"])\n",
    "\n",
    "            file_results[\"objects\"].append({\n",
    "                \"id\": result[\"id\"],\n",
    "                \"center_distance_normalized\": result[\"min_center_distance_normalized\"],\n",
    "                \"size_distance_normalized\": result[\"min_size_distance_normalized\"],\n",
    "                \"avg_corner_error_normalized\": avg_corner_error_normalized,\n",
    "                \"center_distance_non_normalized\": result[\"min_center_distance_non_normalized\"],\n",
    "                \"size_distance_non_normalized\": result[\"min_size_distance_non_normalized\"],\n",
    "                \"avg_corner_error_non_normalized\": avg_corner_error_non_normalized,\n",
    "            })\n",
    "\n",
    "        return file_results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {os.path.basename(image_path)}: {e}\")\n",
    "        return {\"file\": os.path.basename(image_path), \"objects\": []}\n",
    "\n",
    "\n",
    "def process_all_files(model_path, image_dir, ground_truth_dir, max_files):\n",
    "    \"\"\"\n",
    "    Process all images and compare predictions with ground truth.\n",
    "    \n",
    "    \"\"\"\n",
    "    def extract_numeric(file_name):\n",
    "        # Extract the numeric part of the file name (e.g., \"123\" from \"123.png\")\n",
    "        return int(os.path.splitext(file_name)[0])\n",
    "\n",
    "    # Load YOLO model\n",
    "    model = YOLO(model_path)\n",
    "    all_results = []\n",
    "\n",
    "    # List and sort files numerically\n",
    "    image_files = sorted(\n",
    "        [file_name for file_name in os.listdir(image_dir) if file_name.endswith(\".png\")],\n",
    "        key=extract_numeric\n",
    "    )\n",
    "\n",
    "    # Limit the number of files to process if max_files is set\n",
    "    if max_files is not None:\n",
    "        image_files = image_files[:max_files]\n",
    "\n",
    "    for file_name in image_files:\n",
    "        image_path = os.path.join(image_dir, file_name)\n",
    "        gt_file = os.path.join(ground_truth_dir, f\"{os.path.splitext(file_name)[0]}.csv\")\n",
    "\n",
    "        if not os.path.exists(gt_file):\n",
    "            print(f\"Missing ground truth for {file_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Process a single file and collect results\n",
    "        file_results = process_file(model, image_path, gt_file)\n",
    "        all_results.append(file_results)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Process files and collect results\n",
    "results = process_all_files(model_path, image_dir, ground_truth_dir, max_files)\n",
    "\n",
    "print(\"Final Results:\")\n",
    "for file_result in results:\n",
    "    print(file_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_bar_plots(results):\n",
    "    \"\"\"\n",
    "    Generate bar plots from the processed results.\n",
    "    Each bar represents one file with center distance and average corner error.\n",
    "    Separate plots for each metric are also created.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    file_names = []\n",
    "    center_distances = []\n",
    "    avg_corner_errors = []\n",
    "\n",
    "    for file_result in results:\n",
    "        file_names.append(file_result[\"file\"])\n",
    "        total_center_distance = 0\n",
    "        total_avg_corner_error = 0\n",
    "        count = len(file_result[\"objects\"])\n",
    "        \n",
    "        # Sum distances across objects\n",
    "        for obj in file_result[\"objects\"]:\n",
    "            total_center_distance += obj[\"center_distance_non_normalized\"]\n",
    "            total_avg_corner_error += obj[\"avg_corner_error_non_normalized\"]\n",
    "\n",
    "        # Average values per file\n",
    "        center_distances.append(total_center_distance / count if count > 0 else 0)\n",
    "        avg_corner_errors.append(total_avg_corner_error / count if count > 0 else 0)\n",
    "\n",
    "    # Calculate averages for lines\n",
    "    avg_center = np.mean(center_distances)\n",
    "    avg_corner = np.mean(avg_corner_errors)\n",
    "    \n",
    "    x = np.arange(len(file_names))\n",
    "\n",
    "    # Individual Plot: Center Distance with Average Line\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.bar(x + starting_index, center_distances, color=\"skyblue\", label=\"Center Distance\")\n",
    "    plt.axhline(avg_center, color=\"black\", linestyle=\"--\", label=f\"Overall Avarage ({avg_center:.2f})\")\n",
    "    plt.xlabel(\"Files\")\n",
    "    plt.ylabel(\"Center Distance in Pixel Coordinates\")\n",
    "    plt.title(\"Center Distance in Pixel Coordinates for Each File\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_center_distance.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Individual Plot: Average Corner Distance with Average Line\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.bar(x + starting_index, avg_corner_errors, color=\"lightcoral\", label=\"Average Corner Distance\")\n",
    "    plt.axhline(avg_corner, color=\"black\", linestyle=\"--\", label=f\"Overall Avarage ({avg_corner:.2f})\")\n",
    "    plt.xlabel(\"Files\")\n",
    "    plt.ylabel(\"Average Corner Distance in Pixel Coordinates\")\n",
    "    plt.title(\"Average Corner Distance in Pixel Coordinates for Each File\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_corner_distance.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plots\n",
    "generate_bar_plots(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert results to a DataFrame for analysis\n",
    "def process_data(results, metric):\n",
    "    data = []\n",
    "    for file_result in results:\n",
    "        file_name = file_result[\"file\"]\n",
    "        avg_metric_value = np.mean([\n",
    "            obj.get(metric, 0) for obj in file_result[\"objects\"]\n",
    "        ]) if file_result[\"objects\"] else 0\n",
    "        data.append({\"File\": file_name, metric: avg_metric_value})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to identify outliers, average, median, and mode\n",
    "def identify_statistics_euc(df, metric):\n",
    "    top_10_highest = df.nlargest(10, metric)\n",
    "    top_10_lowest = df.nsmallest(10, metric)\n",
    "    median_value = df[metric].median()\n",
    "    avg_value = df[metric].mean()\n",
    "    mode_value = df[metric].mode().iloc[0] if not df[metric].mode().empty else None\n",
    "    return top_10_highest, top_10_lowest, avg_value, median_value, mode_value\n",
    "\n",
    "# Visualization function for center_distance_non_normalized\n",
    "def visualize_center_distance_non_normalized(results):\n",
    "    metric = \"center_distance_non_normalized\"\n",
    "    df = process_data(results, metric)\n",
    "    top_10_high, top_10_low, avg_value, median_value, mode_value = identify_statistics_euc(df, metric)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    combined = pd.concat([top_10_high, top_10_low])\n",
    "    labels = combined[\"File\"]\n",
    "    values = combined[metric]\n",
    "    colors = ['#8fd3d1'] * len(top_10_high) + ['#d1b3f0'] * len(top_10_low)\n",
    "\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    plt.axhline(avg_value, color='red', linestyle='--', label=f'Average ({avg_value:.2f})')\n",
    "    plt.axhline(median_value, color='blue', linestyle='-.', label=f'Median ({median_value:.2f})')\n",
    "    if mode_value is not None:\n",
    "        plt.axhline(mode_value, color='green', linestyle=':', label=f'Mode ({mode_value:.2f})')\n",
    "\n",
    "    plt.xlabel(\"CSV Files\")\n",
    "    plt.ylabel(\"Center Distance in Pixel Coordinates \")\n",
    "    plt.title(\"Statistics for Center Distance in Pixel Coordinates\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_center_distance_statistics.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Visualization function for avg_corner_error_non_normalized\n",
    "def visualize_avg_corner_error_non_normalized(results):\n",
    "    metric = \"avg_corner_error_non_normalized\"\n",
    "    df = process_data(results, metric)\n",
    "    top_10_high, top_10_low, avg_value, median_value, mode_value = identify_statistics_euc(df, metric)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    combined = pd.concat([top_10_high, top_10_low])\n",
    "    labels = combined[\"File\"]\n",
    "    values = combined[metric]\n",
    "    colors = ['#8fd3d1'] * len(top_10_high) + ['#d1b3f0'] * len(top_10_low)\n",
    "\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    plt.axhline(avg_value, color='red', linestyle='--', label=f'Average ({avg_value:.2f})')\n",
    "    plt.axhline(median_value, color='blue', linestyle='-.', label=f'Median ({median_value:.2f})')\n",
    "    if mode_value is not None:\n",
    "        plt.axhline(mode_value, color='green', linestyle=':', label=f'Mode ({mode_value:.2f})')\n",
    "\n",
    "    plt.xlabel(\"CSV Files\")\n",
    "    plt.ylabel(\"Average Corner Distance in Pixel Coordinates\")\n",
    "    plt.title(\"Statistics for Average Corner Distance in Pixel Coordinates\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_corner_distance_statistics.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Generate individual plots\n",
    "visualize_center_distance_non_normalized(results)\n",
    "visualize_avg_corner_error_non_normalized(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Detections and Not Detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fp_and_nd(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate False Positives (FP) and Not Detected (ND).\n",
    "    \"\"\"\n",
    "    # If either DataFrame is empty, handle gracefully\n",
    "    if predictions.empty and ground_truth.empty:\n",
    "        return 0, 0\n",
    "    elif predictions.empty:\n",
    "        return 0, len(ground_truth)\n",
    "    elif ground_truth.empty:\n",
    "        return len(predictions), 0\n",
    "\n",
    "    # Ensure the required column exists\n",
    "    if 0 not in predictions.columns or 0 not in ground_truth.columns:\n",
    "        print(\"Error: Column 0 (Object ID) is missing in one of the DataFrames.\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Group ground truth and predictions by object ID\n",
    "    gt_groups = ground_truth.groupby(0)\n",
    "    pred_groups = predictions.groupby(0)\n",
    "\n",
    "    # Unique IDs in predictions and ground truth\n",
    "    gt_ids = set(gt_groups.groups.keys())\n",
    "    pred_ids = set(pred_groups.groups.keys())\n",
    "\n",
    "    # Initialize counters for FP and ND\n",
    "    false_detections = 0\n",
    "    not_detected = 0\n",
    "\n",
    "    # IDs only in ground truth (ND)\n",
    "    unique_gt_ids = gt_ids - pred_ids\n",
    "    not_detected += sum(len(gt_groups.get_group(id)) for id in unique_gt_ids)\n",
    "\n",
    "    # IDs only in predictions (FP)\n",
    "    unique_pred_ids = pred_ids - gt_ids\n",
    "    false_detections += sum(len(pred_groups.get_group(id)) for id in unique_pred_ids)\n",
    "\n",
    "    # IDs present in both\n",
    "    common_ids = gt_ids & pred_ids\n",
    "    for obj_id in common_ids:\n",
    "        gt_count = len(gt_groups.get_group(obj_id))\n",
    "        pred_count = len(pred_groups.get_group(obj_id))\n",
    "\n",
    "        # Count mismatched instances\n",
    "        if pred_count > gt_count:\n",
    "            false_detections += pred_count - gt_count\n",
    "        elif gt_count > pred_count:\n",
    "            not_detected += gt_count - pred_count\n",
    "\n",
    "    return false_detections, not_detected\n",
    "\n",
    "\n",
    "def process_file_fp_nd(model, image_path, gt_file):\n",
    "    \"\"\"\n",
    "    Process a single file and calculate False Positives (FP) and Not Detected (ND).\n",
    "    \"\"\"\n",
    "    # Perform YOLO inference\n",
    "    results = model(image_path)[0]\n",
    "\n",
    "    # Get image dimensions\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "\n",
    "    # Prepare prediction data\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        # Object ID\n",
    "        obj_id = int(result.boxes.cls[0])  # Class ID\n",
    "\n",
    "        # Bounding box center and size\n",
    "        box = result.boxes.xyxy[0].tolist()\n",
    "        center_x = (box[0] + box[2]) / 2 / width\n",
    "        center_y = (box[1] + box[3]) / 2 / height\n",
    "        size_x = (box[2] - box[0]) / width\n",
    "        size_y = (box[3] - box[1]) / height\n",
    "\n",
    "        # Keypoints (if available)\n",
    "        keypoints = []\n",
    "        if result.keypoints is not None:\n",
    "            keypoints = result.keypoints.xy[0].tolist()\n",
    "            keypoints = [(kp[0] / width, kp[1] / height) for kp in keypoints[:8]]\n",
    "\n",
    "        # Prepare prediction row\n",
    "        row = [obj_id, center_x, center_y, size_x, size_y]\n",
    "        row.extend(sum(keypoints, ()))\n",
    "        predictions.append(row)\n",
    "\n",
    "    # Convert predictions to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "    # Load ground truth\n",
    "    ground_truth = pd.read_csv(gt_file, header=None)\n",
    "\n",
    "    # Handle empty DataFrames gracefully\n",
    "    if predictions_df.empty or ground_truth.empty:\n",
    "        print(f\"Skipping file {os.path.basename(image_path)} due to empty DataFrames.\")\n",
    "        return {\n",
    "            \"file\": os.path.basename(image_path),\n",
    "            \"false_detections\": len(predictions_df),\n",
    "            \"not_detected\": len(ground_truth)\n",
    "        }\n",
    "\n",
    "    # Calculate FP and ND\n",
    "    false_detections, not_detected = calculate_fp_and_nd(predictions_df, ground_truth)\n",
    "\n",
    "    return {\n",
    "        \"file\": os.path.basename(image_path),\n",
    "        \"false_detections\": false_detections,\n",
    "        \"not_detected\": not_detected,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_all_files_fp_nd(model_path, image_dir, ground_truth_dir, max_files):\n",
    "    \"\"\"\n",
    "    Process all files to calculate False Positives (FP) and Not Detected (ND).\n",
    "\n",
    "    \"\"\"\n",
    "    def extract_numeric(file_name):\n",
    "        # Extract the numeric part of the file name (e.g., \"123\" from \"123.png\")\n",
    "        return int(os.path.splitext(file_name)[0])\n",
    "\n",
    "    # Load YOLO model\n",
    "    model = YOLO(model_path)\n",
    "    results = []\n",
    "\n",
    "    # List and sort files numerically\n",
    "    image_files = sorted(\n",
    "        [file_name for file_name in os.listdir(image_dir) if file_name.endswith(\".png\")],\n",
    "        key=extract_numeric\n",
    "    )\n",
    "\n",
    "    # Limit the number of files to process if max_files is set\n",
    "    if max_files is not None:\n",
    "        image_files = image_files[:max_files]\n",
    "\n",
    "    for file_name in image_files:\n",
    "        # Image file name without extension\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "        # Paths\n",
    "        image_path = os.path.join(image_dir, file_name)\n",
    "        gt_file = os.path.join(ground_truth_dir, f\"{base_name}.csv\")\n",
    "\n",
    "        if not os.path.exists(gt_file):\n",
    "            print(f\"Missing ground truth for {base_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Process a single file\n",
    "        try:\n",
    "            file_results = process_file_fp_nd(model, image_path, gt_file)\n",
    "            results.append({\n",
    "                \"file\": file_results[\"file\"],\n",
    "                \"false_detections\": file_results[\"false_detections\"],\n",
    "                \"not_detected\": file_results[\"not_detected\"]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Calculate FP and ND\n",
    "results = process_all_files_fp_nd(model_path, image_dir, ground_truth_dir, max_files)\n",
    "\n",
    "print(\"Results:\")\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate bar plots with updated results\n",
    "def generate_fp_nd_bar_plots(results):\n",
    "    \"\"\"\n",
    "    Generate bar plots for False Positives (FP) and Not Detected (ND).\n",
    "    Each bar represents one file with FP and ND stacked.\n",
    "    Also generates individual plots for FP and ND with average lines.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    file_names = [result[\"file\"] for result in results]\n",
    "    false_detections = [result[\"false_detections\"] for result in results]\n",
    "    not_detected = [result[\"not_detected\"] for result in results]\n",
    "\n",
    "    # Calculate averages for lines\n",
    "    avg_fp = np.mean(false_detections)\n",
    "    avg_nd = np.mean(not_detected)\n",
    "    \n",
    "    x = np.arange(len(file_names))\n",
    "\n",
    "    # Stacked Bar Plot: FP and ND\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.bar(x + starting_index, false_detections, label=\"False Detections (FD)\", color=\"skyblue\")\n",
    "    plt.bar(x + starting_index, not_detected, bottom=false_detections, label=\"Not Detected (ND)\", color=\"lightcoral\")\n",
    "    plt.axhline(avg_fp + avg_nd, color=\"black\", linestyle=\"--\", label=f\"Overall Avarage ({avg_fp + avg_nd:.2f})\")\n",
    "    plt.xlabel(\"Files\")\n",
    "    plt.ylabel(\"False Detections and Not Detected Objects\")\n",
    "    plt.title(\"False Detections and Not Detected for Each File\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_false_detections_and_not_detected.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Individual Plot: False Positives with Average Line\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.bar(x + starting_index, false_detections, color=\"skyblue\", label=\"False Detections (FD)\")\n",
    "    plt.axhline(avg_fp, color=\"black\", linestyle=\"--\", label=f\"Overall Avarage ({avg_fp:.2f})\")\n",
    "    plt.xlabel(\"Files\")\n",
    "    plt.ylabel(\"False Detections\")\n",
    "    plt.title(\"False Detections for Each File\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_false_detections.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Individual Plot: Not Detected with Average Line\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.bar(x + starting_index, not_detected, color=\"lightcoral\", label=\"Not Detected (ND)\")\n",
    "    plt.axhline(avg_nd, color=\"black\", linestyle=\"--\", label=f\"Overall Avarage ({avg_nd:.2f})\")\n",
    "    plt.xlabel(\"Files\")\n",
    "    plt.ylabel(\"Not Detected Objects\")\n",
    "    plt.title(\"Not Detected for Each File\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_not_detected.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate the plots\n",
    "generate_fp_nd_bar_plots(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert FP and ND results to a DataFrame for analysis\n",
    "def process_fp_nd_results(results):\n",
    "    \"\"\"\n",
    "    Process results to calculate false detections, not detected, and their combined sum.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for result in results:\n",
    "        file_name = result[\"file\"]\n",
    "        false_detections = result.get(\"false_detections\", 0)\n",
    "        not_detected = result.get(\"not_detected\", 0)\n",
    "        combined_metric = false_detections + not_detected\n",
    "        data.append({\n",
    "            \"File\": file_name,\n",
    "            \"false_detections\": false_detections,\n",
    "            \"not_detected\": not_detected,\n",
    "            \"combined_metric\": combined_metric\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Identify statistics for a given metric\n",
    "def identify_statistics_fd_nd(df, metric):\n",
    "    \"\"\"\n",
    "    Identify statistical measures for the specified metric.\n",
    "    \"\"\"\n",
    "    top_10_highest = df.nlargest(10, metric)\n",
    "    top_10_lowest = df.nsmallest(10, metric)\n",
    "    median_value = df[metric].median()\n",
    "    avg_value = df[metric].mean()\n",
    "    mode_value = df[metric].mode().iloc[0] if not df[metric].mode().empty else None\n",
    "    return top_10_highest, top_10_lowest, avg_value, median_value, mode_value\n",
    "\n",
    "# Visualize false detections\n",
    "def visualize_false_detections(df, save_dir, dataset):\n",
    "    \"\"\"\n",
    "    Visualize statistics for false detections.\n",
    "    \"\"\"\n",
    "    metric = \"false_detections\"\n",
    "    top_10_high, top_10_low, avg_value, median_value, mode_value = identify_statistics_fd_nd(df, metric)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    combined = pd.concat([top_10_high, top_10_low])\n",
    "    labels = combined[\"File\"]\n",
    "    values = combined[metric]\n",
    "    colors = ['#8fd3d1'] * len(top_10_high) + ['#d1b3f0'] * len(top_10_low)\n",
    "\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    plt.axhline(avg_value, color='red', linestyle='--', label=f'Average ({avg_value:.2f})')\n",
    "    plt.axhline(median_value, color='blue', linestyle='-.', label=f'Median ({median_value:.2f})')\n",
    "    if mode_value is not None:\n",
    "        plt.axhline(mode_value, color='green', linestyle=':', label=f'Mode ({mode_value:.2f})')\n",
    "\n",
    "    plt.xlabel(\"CSV Files\")\n",
    "    plt.ylabel(\"False Detections\")\n",
    "    plt.title(\"Statistics for False Detections\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/yolo_{dataset}_{metric}_statistics.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize not detected\n",
    "def visualize_not_detected(df, save_dir, dataset):\n",
    "    \"\"\"\n",
    "    Visualize statistics for not detected.\n",
    "    \"\"\"\n",
    "    metric = \"not_detected\"\n",
    "    top_10_high, top_10_low, avg_value, median_value, mode_value = identify_statistics_fd_nd(df, metric)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    combined = pd.concat([top_10_high, top_10_low])\n",
    "    labels = combined[\"File\"]\n",
    "    values = combined[metric]\n",
    "    colors = ['#8fd3d1'] * len(top_10_high) + ['#d1b3f0'] * len(top_10_low)\n",
    "\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    plt.axhline(avg_value, color='red', linestyle='--', label=f'Average ({avg_value:.2f})')\n",
    "    plt.axhline(median_value, color='blue', linestyle='-.', label=f'Median ({median_value:.2f})')\n",
    "    if mode_value is not None:\n",
    "        plt.axhline(mode_value, color='green', linestyle=':', label=f'Mode ({mode_value:.2f})')\n",
    "\n",
    "    plt.xlabel(\"CSV Files\")\n",
    "    plt.ylabel(\"Not Detected Objects\")\n",
    "    plt.title(\"Statistics for Not Detected Objects\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/yolo_{dataset}_{metric}_statistics.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize combined metric\n",
    "def visualize_combined_metric(df, save_dir, dataset):\n",
    "    \"\"\"\n",
    "    Visualize statistics for the combined metric (false detections + not detected).\n",
    "    \"\"\"\n",
    "    metric = \"combined_metric\"\n",
    "    top_10_high, top_10_low, avg_value, median_value, mode_value = identify_statistics_fd_nd(df, metric)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    combined = pd.concat([top_10_high, top_10_low])\n",
    "    labels = combined[\"File\"]\n",
    "    values = combined[metric]\n",
    "    colors = ['#8fd3d1'] * len(top_10_high) + ['#d1b3f0'] * len(top_10_low)\n",
    "\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    plt.axhline(avg_value, color='red', linestyle='--', label=f'Average ({avg_value:.2f})')\n",
    "    plt.axhline(median_value, color='blue', linestyle='-.', label=f'Median ({median_value:.2f})')\n",
    "    if mode_value is not None:\n",
    "        plt.axhline(mode_value, color='green', linestyle=':', label=f'Mode ({mode_value:.2f})')\n",
    "\n",
    "    plt.xlabel(\"CSV Files\")\n",
    "    plt.ylabel(\"False Detections and Not Detected Objects\")\n",
    "    plt.title(\"Statistics for False Detections and Not Detected Objects\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/yolo_{dataset}_false_detections_and_not_detected_statistics.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Process results and generate plots\n",
    "df = process_fp_nd_results(results)\n",
    "\n",
    "# Generate separate plots\n",
    "visualize_false_detections(df, save_dir, dataset)\n",
    "visualize_not_detected(df, save_dir, dataset)\n",
    "visualize_combined_metric(df, save_dir, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def compute_intersection_area_ratio(gt_row, pred_row, width, height):\n",
    "    \"\"\"\n",
    "    Computes the intersection area ratio of two bounding boxes (ground truth and prediction).\n",
    "    \"\"\"\n",
    "    # Ground truth bounding box edges\n",
    "    gt_center_x = gt_row[1] * width\n",
    "    gt_center_y = gt_row[2] * height\n",
    "    gt_bbox_width = gt_row[3] * width\n",
    "    gt_bbox_height = gt_row[4] * height\n",
    "\n",
    "    gt_x1 = gt_center_x - (gt_bbox_width / 2)\n",
    "    gt_y1 = gt_center_y - (gt_bbox_height / 2)\n",
    "    gt_x2 = gt_center_x + (gt_bbox_width / 2)\n",
    "    gt_y2 = gt_center_y + (gt_bbox_height / 2)\n",
    "\n",
    "    # Prediction bounding box edges\n",
    "    pred_center_x = pred_row[1] * width\n",
    "    pred_center_y = pred_row[2] * height\n",
    "    pred_bbox_width = pred_row[3] * width\n",
    "    pred_bbox_height = pred_row[4] * height\n",
    "\n",
    "    pred_x1 = pred_center_x - (pred_bbox_width / 2)\n",
    "    pred_y1 = pred_center_y - (pred_bbox_height / 2)\n",
    "    pred_x2 = pred_center_x + (pred_bbox_width / 2)\n",
    "    pred_y2 = pred_center_y + (pred_bbox_height / 2)\n",
    "\n",
    "    # Calculate the intersection box\n",
    "    inter_x1 = max(gt_x1, pred_x1)\n",
    "    inter_y1 = max(gt_y1, pred_y1)\n",
    "    inter_x2 = min(gt_x2, pred_x2)\n",
    "    inter_y2 = min(gt_y2, pred_y2)\n",
    "\n",
    "    # Calculate intersection area\n",
    "    inter_width = max(0, inter_x2 - inter_x1)\n",
    "    inter_height = max(0, inter_y2 - inter_y1)\n",
    "    intersection_area = inter_width * inter_height\n",
    "\n",
    "    # Calculate ground truth area\n",
    "    gt_area = max(0, (gt_x2 - gt_x1) * (gt_y2 - gt_y1))\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if gt_area == 0:\n",
    "        return 0\n",
    "\n",
    "    # Return the intersection area ratio\n",
    "    return intersection_area / gt_area\n",
    "\n",
    "\n",
    "\n",
    "def calculate_intersection_ratios(predictions, ground_truth, width, height):\n",
    "    \"\"\"\n",
    "    Calculate the intersection area ratios for matched predictions and ground truths.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    if predictions.empty or ground_truth.empty:\n",
    "        print(\"One of the DataFrames is empty, skipping...\")\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        # Group ground truth and predictions by object ID\n",
    "        gt_groups = ground_truth.groupby(0)\n",
    "        pred_groups = predictions.groupby(0)\n",
    "\n",
    "        # Get the intersection of IDs present in both predictions and ground truth\n",
    "        common_ids = set(gt_groups.groups.keys()) & set(pred_groups.groups.keys())\n",
    "\n",
    "        for obj_id in common_ids:\n",
    "            gt_rows = ground_truth.loc[gt_groups.groups[obj_id]].copy()\n",
    "            pred_rows = predictions.loc[pred_groups.groups[obj_id]].copy()\n",
    "\n",
    "            # Match predictions and ground truth for this object ID\n",
    "            while not gt_rows.empty and not pred_rows.empty:\n",
    "                max_ratio = None\n",
    "                max_gt_index = None\n",
    "                max_pred_index = None\n",
    "\n",
    "                # Find the prediction-ground truth pair with the highest intersection ratio\n",
    "                for gt_index, gt_row in gt_rows.iterrows():\n",
    "                    for pred_index, pred_row in pred_rows.iterrows():\n",
    "                        ratio = compute_intersection_area_ratio(gt_row, pred_row, width, height)\n",
    "                        if max_ratio is None or ratio > max_ratio:\n",
    "                            max_ratio = ratio\n",
    "                            max_gt_index = gt_index\n",
    "                            max_pred_index = pred_index\n",
    "\n",
    "                # Save the intersection ratio result\n",
    "                results.append({\n",
    "                    \"id\": obj_id,\n",
    "                    \"intersection_ratio\": max_ratio\n",
    "                })\n",
    "\n",
    "                # Remove matched rows\n",
    "                gt_rows.drop(index=max_gt_index, inplace=True)\n",
    "                pred_rows.drop(index=max_pred_index, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating intersection ratios: {e}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_file_intersection(model, image_path, gt_file):\n",
    "    \"\"\"\n",
    "    Process a single image and calculate intersection area ratios.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform YOLO inference\n",
    "        results = model(image_path)[0]\n",
    "\n",
    "        # Get image dimensions\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "\n",
    "        # Prepare prediction data\n",
    "        predictions = []\n",
    "        for result in results:\n",
    "            # Object ID\n",
    "            obj_id = int(result.boxes.cls[0])  # Class ID\n",
    "\n",
    "            # Bounding box center and size\n",
    "            box = result.boxes.xyxy[0].tolist()\n",
    "            center_x = (box[0] + box[2]) / 2 / width\n",
    "            center_y = (box[1] + box[3]) / 2 / height\n",
    "            size_x = (box[2] - box[0]) / width\n",
    "            size_y = (box[3] - box[1]) / height\n",
    "\n",
    "            # Prepare prediction row\n",
    "            row = [obj_id, center_x, center_y, size_x, size_y]\n",
    "            predictions.append(row)\n",
    "\n",
    "        # Convert predictions to DataFrame\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "        # Load ground truth\n",
    "        ground_truth = pd.read_csv(gt_file, header=None)\n",
    "\n",
    "        # Ensure DataFrames are not empty\n",
    "        if predictions_df.empty or ground_truth.empty:\n",
    "            print(f\"Empty DataFrame encountered for {os.path.basename(image_path)}, skipping...\")\n",
    "            return {\"file\": os.path.basename(image_path), \"objects\": []}\n",
    "\n",
    "        # Calculate intersection ratios\n",
    "        results = calculate_intersection_ratios(predictions_df, ground_truth, width, height)\n",
    "\n",
    "        # Prepare results for the file\n",
    "        file_results = {\"file\": os.path.basename(image_path), \"objects\": results}\n",
    "        return file_results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {os.path.basename(image_path)}: {e}\")\n",
    "        return {\"file\": os.path.basename(image_path), \"objects\": []}\n",
    "\n",
    "\n",
    "def process_all_files_intersection(model_path, image_dir, ground_truth_dir, max_files):\n",
    "    \"\"\"\n",
    "    Process all images and calculate intersection area ratios.\n",
    "    \"\"\"\n",
    "    def extract_numeric(file_name):\n",
    "        # Extract the numeric part of the file name (e.g., \"123\" from \"123.png\")\n",
    "        return int(os.path.splitext(file_name)[0])\n",
    "\n",
    "    # Load YOLO model\n",
    "    model = YOLO(model_path)\n",
    "    all_results = []\n",
    "\n",
    "    # List and sort files numerically\n",
    "    image_files = sorted(\n",
    "        [file_name for file_name in os.listdir(image_dir) if file_name.endswith(\".png\")],\n",
    "        key=extract_numeric\n",
    "    )\n",
    "\n",
    "    # Limit the number of files to process if max_files is set\n",
    "    if max_files is not None:\n",
    "        image_files = image_files[:max_files]\n",
    "\n",
    "    for file_name in image_files:\n",
    "        image_path = os.path.join(image_dir, file_name)\n",
    "        gt_file = os.path.join(ground_truth_dir, f\"{os.path.splitext(file_name)[0]}.csv\")\n",
    "\n",
    "        if not os.path.exists(gt_file):\n",
    "            print(f\"Missing ground truth for {file_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Process a single file and collect results\n",
    "        try:\n",
    "            file_results = process_file_intersection(model, image_path, gt_file)\n",
    "            all_results.append(file_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Process files and collect results\n",
    "results = process_all_files_intersection(model_path, image_dir, ground_truth_dir, max_files)\n",
    "\n",
    "print(\"Intersection Area Ratios:\")\n",
    "for file_result in results:\n",
    "    print(file_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_average_intersection_plot(results):\n",
    "    \"\"\"\n",
    "    Generate a bar plot showing the average intersection percentage for each file\n",
    "    and a line for the overall average percentage.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    file_names = [result.get(\"file\", \"Unknown\") for result in results]\n",
    "    avg_ratios_per_file = [\n",
    "        100 * np.mean([obj.get(\"intersection_ratio\", 0) for obj in result.get(\"objects\", [])]) if result.get(\"objects\") else 0\n",
    "        for result in results\n",
    "    ]\n",
    "    overall_avg_ratio = np.mean([ratio for ratio in avg_ratios_per_file if ratio > 0])\n",
    "\n",
    "    x = np.arange(len(file_names))\n",
    "\n",
    "    # Bar Plot: Average Ratios Per File\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.bar(x + starting_index, avg_ratios_per_file, color=\"lightblue\", label=\"Average Intersection % Per File\")\n",
    "    plt.axhline(overall_avg_ratio, color=\"black\", linestyle=\"--\", label=f\"Overall Avarage ({overall_avg_ratio:.2f}%)\")\n",
    "    plt.xlabel(\"Files\")\n",
    "    plt.ylabel(\"Average Intersection Percentage (%)\")\n",
    "    plt.title(\"Average Intersection Percentages for Each File\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_average_intersection_percentage.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plot\n",
    "generate_average_intersection_plot(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def generate_intersection_plot_per_id(results):\n",
    "    \"\"\"\n",
    "    Generate bar plots showing the average intersection percentage for each object ID across files.\n",
    "    \"\"\"\n",
    "    # Collect data per ID\n",
    "    id_data = defaultdict(list)  # {id: [ratios]}\n",
    "    for result in results:\n",
    "        for obj in result.get(\"objects\", []):\n",
    "            obj_id = obj.get(\"id\")\n",
    "            intersection_ratio = 100 * obj.get(\"intersection_ratio\", 0)  # Convert to percentage\n",
    "            id_data[obj_id].append(intersection_ratio)\n",
    "\n",
    "    # Calculate average ratios per ID\n",
    "    avg_ratios_per_id = {obj_id: np.mean(ratios) for obj_id, ratios in id_data.items() if ratios}\n",
    "\n",
    "    # Sort IDs for consistent plotting\n",
    "    sorted_ids = sorted(avg_ratios_per_id.keys())\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    avg_ratios = [avg_ratios_per_id[obj_id] for obj_id in sorted_ids]\n",
    "\n",
    "    x = np.arange(len(sorted_ids))\n",
    "\n",
    "    # Bar Plot: Average Intersection Ratios Per ID\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.bar(x, avg_ratios, color=\"lightgreen\", label=\"Average Intersection % Per ID\")\n",
    "    overall_avg_ratio = np.mean(avg_ratios)\n",
    "    plt.axhline(overall_avg_ratio, color=\"black\", linestyle=\"--\", label=f\"Overall Avarage ({overall_avg_ratio:.2f}%)\")\n",
    "    plt.xticks(x, sorted_ids, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Object IDs\")\n",
    "    plt.ylabel(\"Average Intersection Percentage (%)\")\n",
    "    plt.title(\"Average Intersection Percentages for Each Object ID\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}\\\\yolo_{dataset}_average_intersection_percentage_object_id.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate the plot for IDs\n",
    "generate_intersection_plot_per_id(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Process intersection area ratio results into a DataFrame\n",
    "def process_intersection_ratios(results):\n",
    "    \"\"\"\n",
    "    Process results to calculate the intersection area ratio for each file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for result in results:\n",
    "        file_name = result[\"file\"]\n",
    "        avg_intersection_ratio = np.mean([obj.get(\"intersection_ratio\", 0) for obj in result[\"objects\"]])\n",
    "        data.append({\n",
    "            \"File\": file_name,\n",
    "            \"intersection_ratio\": avg_intersection_ratio\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Identify statistics for intersection area ratios\n",
    "def identify_statistics_intersection(df, metric):\n",
    "    \"\"\"\n",
    "    Identify statistical measures for intersection area ratios.\n",
    "    \"\"\"\n",
    "    top_10_highest = df.nlargest(10, metric)\n",
    "    top_10_lowest = df.nsmallest(10, metric)\n",
    "    median_value = df[metric].median()\n",
    "    avg_value = df[metric].mean()\n",
    "    mode_value = df[metric].mode().iloc[0] if not df[metric].mode().empty else None\n",
    "    return top_10_highest, top_10_lowest, avg_value, median_value, mode_value\n",
    "\n",
    "# Visualize intersection ratios\n",
    "def visualize_intersection_ratios(df, save_dir, dataset):\n",
    "    \"\"\"\n",
    "    Visualize statistics for intersection area ratios.\n",
    "    \"\"\"\n",
    "    metric = \"intersection_ratio\"\n",
    "    top_10_high, top_10_low, avg_value, median_value, mode_value = identify_statistics_intersection(df, metric)\n",
    "\n",
    "    plt.figure(figsize=(14,8))\n",
    "    combined = pd.concat([top_10_high, top_10_low])\n",
    "    labels = combined[\"File\"]\n",
    "    values = combined[metric]\n",
    "    colors = ['#8fd3d1'] * len(top_10_high) + ['#d1b3f0'] * len(top_10_low)\n",
    "\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    plt.axhline(avg_value, color='red', linestyle='--', label=f'Average ({avg_value:.2f})')\n",
    "    plt.axhline(median_value, color='blue', linestyle='-.', label=f'Median ({median_value:.2f})')\n",
    "    if mode_value is not None:\n",
    "        plt.axhline(mode_value, color='green', linestyle=':', label=f'Mode ({mode_value:.2f})')\n",
    "\n",
    "    plt.xlabel(\"CSV Files\")\n",
    "    plt.ylabel(\"Average Intersection Percentage (%)\")\n",
    "    plt.title(\"Statistics for Average Intersection Percentages\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/yolo_{dataset}_average_intersection_percentage_statistic.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Process results into a DataFrame\n",
    "df = process_intersection_ratios(results)\n",
    "\n",
    "# Visualize the statistics\n",
    "visualize_intersection_ratios(df, save_dir, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def draw_combined_visualization_with_black_canvas(model_path, image_path, data_2d_path):\n",
    "    # Load the YOLO model and perform inference\n",
    "    model = YOLO(model_path)\n",
    "    results = model(image_path)[0]\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    # Create a black canvas\n",
    "    black_canvas = Image.new(\"RGB\", (width, height), \"black\")\n",
    "    draw_img = ImageDraw.Draw(img)\n",
    "    draw_black = ImageDraw.Draw(black_canvas)\n",
    "\n",
    "    def draw_yolo_predictions():\n",
    "        \"\"\"Draw YOLO predictions on the image and black canvas.\"\"\"\n",
    "        for result in results:\n",
    "            box = result.boxes.xyxy[0]\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "            # Draw YOLO bounding box in red\n",
    "            for draw in [draw_img, draw_black]:\n",
    "                draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "                draw.ellipse([center_x - 3, center_y - 3, center_x + 3, center_y + 3], fill=\"red\", outline=\"red\")\n",
    "\n",
    "            # YOLO keypoints\n",
    "            if result.keypoints:\n",
    "                keypoints = [\n",
    "                    (int(x), int(y)) for x, y in result.keypoints.xy[0]\n",
    "                    if x != 0 and y != 0  # Filter out invalid keypoints\n",
    "                ]\n",
    "                if len(keypoints) >= 8:  # Ensure there are at least 8 keypoints\n",
    "                    top_square = keypoints[:4]\n",
    "                    bottom_square = keypoints[4:8]\n",
    "                    for i, (x, y) in enumerate(keypoints[:8]):\n",
    "                        for draw in [draw_img, draw_black]:\n",
    "                            draw.ellipse([x - 3, y - 3, x + 3, y + 3], fill=\"red\", outline=\"red\")\n",
    "                            draw.text((x + 5, y + 5), str(i), fill=\"white\", font=font)\n",
    "                    for i in range(4):\n",
    "                        for draw in [draw_img, draw_black]:\n",
    "                            draw.line([top_square[i], top_square[(i + 1) % 4]], fill=\"orange\", width=2)\n",
    "                            draw.line([bottom_square[i], bottom_square[(i + 1) % 4]], fill=\"orange\", width=2)\n",
    "                            draw.line([top_square[i], bottom_square[i]], fill=\"orange\", width=2)\n",
    "                else:\n",
    "                    print(f\"Skipping keypoints due to insufficient data: {keypoints}\")\n",
    "\n",
    "    def draw_2d_data():\n",
    "        \"\"\"Draw 2D data bounding boxes and corner points.\"\"\"\n",
    "        if os.path.exists(data_2d_path):\n",
    "            try:\n",
    "                df = pd.read_csv(data_2d_path, header=None)\n",
    "                for _, row in df.iterrows():\n",
    "                    center_x = row[1] * width\n",
    "                    center_y = row[2] * height\n",
    "                    bbox_width = row[3] * width\n",
    "                    bbox_height = row[4] * height\n",
    "\n",
    "                    top_left_x = center_x - (bbox_width / 2)\n",
    "                    top_left_y = center_y - (bbox_height / 2)\n",
    "                    bottom_right_x = center_x + (bbox_width / 2)\n",
    "                    bottom_right_y = center_y + (bbox_height / 2)\n",
    "\n",
    "                    # Draw 2D bounding box in blue\n",
    "                    for draw in [draw_img, draw_black]:\n",
    "                        draw.rectangle([top_left_x, top_left_y, bottom_right_x, bottom_right_y], outline=\"blue\", width=2)\n",
    "                        draw.ellipse([center_x - 3, center_y - 3, center_x + 3, center_y + 3], fill=\"blue\", outline=\"blue\")\n",
    "\n",
    "                    # Draw corner points if present\n",
    "                    if len(row) >= 20:\n",
    "                        corner_points = [\n",
    "                            (row[i] * width, row[i + 1] * height) for i in range(5, 20, 2)\n",
    "                            if not (row[i] == 0 and row[i + 1] == 0)  # Filter out invalid points\n",
    "                        ]\n",
    "                        if len(corner_points) >= 8:\n",
    "                            top_square = corner_points[:4]\n",
    "                            bottom_square = corner_points[4:]\n",
    "                            for i, (x, y) in enumerate(corner_points):\n",
    "                                for draw in [draw_img, draw_black]:\n",
    "                                    draw.ellipse([x - 3, y - 3, x + 3, y + 3], fill=\"blue\", outline=\"blue\")\n",
    "                                    draw.text((x + 5, y + 5), str(i), fill=\"white\", font=font)\n",
    "                            for i in range(4):\n",
    "                                for draw in [draw_img, draw_black]:\n",
    "                                    draw.line([top_square[i], top_square[(i + 1) % 4]], fill=\"green\", width=2)\n",
    "                                    draw.line([bottom_square[i], bottom_square[(i + 1) % 4]], fill=\"green\", width=2)\n",
    "                                    draw.line([top_square[i], bottom_square[i]], fill=\"green\", width=2)\n",
    "                        else:\n",
    "                            print(f\"Skipping corners due to insufficient data: {corner_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing 2D data: {e}\")\n",
    "        else:\n",
    "            print(f\"2D data file not found: {data_2d_path}\")\n",
    "\n",
    "    # Draw the visualizations\n",
    "    draw_yolo_predictions()\n",
    "    draw_2d_data()\n",
    "\n",
    "    # Save and show both images\n",
    "    img.show()\n",
    "    black_canvas.show()\n",
    "\n",
    "    output_image_path = os.path.join(save_dir, f\"yolo_{dataset}_combined_visualization_{file_name}.png\")\n",
    "    black_canvas_path = os.path.join(save_dir, f\"yolo_{dataset}_combined_visualization_black_{file_name}.png\")\n",
    "\n",
    "    img.save(output_image_path)\n",
    "    black_canvas.save(black_canvas_path)\n",
    "\n",
    "    print(f\"Annotated image saved to {output_image_path}\")\n",
    "    print(f\"Black canvas visualization saved to {black_canvas_path}\")\n",
    "\n",
    "# Example usage\n",
    "file_name = '10012'\n",
    "image_path = f'C:/Users/sakar/OneDrive/mt-datas/synthetic_data/12_yoro_dataset/images/test/{file_name}.png'\n",
    "data_2d_path = f'C:/Users/sakar/OneDrive/mt-datas/synthetic_data/12_yoro_dataset/2d_data/{file_name}.csv'\n",
    "\n",
    "draw_combined_visualization_with_black_canvas(model_path, image_path, data_2d_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
