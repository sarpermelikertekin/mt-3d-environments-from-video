{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer2DTo3D(nn.Module):\n",
    "    def __init__(self, input_dim=21, output_dim=31, d_model=128, nhead=8, num_encoder_layers=6, dim_feedforward=512):\n",
    "        super(Transformer2DTo3D, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)  # Aggregate sequence for regression\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Save model definition to use in inference\n",
    "print(\"Model definition loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Dummy dataset (Replace this with actual 2D input and 3D target data)\n",
    "input_data = torch.randn(1000, 10, 21)  # 1000 samples, sequence length 10, 21 features\n",
    "target_data = torch.randn(1000, 31)     # Corresponding 3D outputs\n",
    "\n",
    "# Dataloader\n",
    "dataset = TensorDataset(input_data, target_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, loss\n",
    "model = Transformer2DTo3D(input_dim=21, output_dim=31)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"transformer_2d_to_3d_v2.pth\")\n",
    "print(\"Model training completed and weights saved to 'transformer_2d_to_3d_v2.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = Transformer2DTo3D(input_dim=21, output_dim=31)\n",
    "model.load_state_dict(torch.load(\"transformer_2d_to_3d_v2.pth\"))\n",
    "model.eval()\n",
    "print(\"Model weights loaded for inference.\")\n",
    "\n",
    "# Inference function\n",
    "def run_inference(model, input_data):\n",
    "    \"\"\"\n",
    "    Runs inference using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model (Transformer2DTo3D): Trained model.\n",
    "        input_data (torch.Tensor): 2D input data (batch_size, seq_len, input_dim).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Predicted 3D outputs.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_data)\n",
    "    return predictions\n",
    "\n",
    "# Example inference (Replace `test_input` with actual test data)\n",
    "test_input = torch.randn(5, 10, 21)  # 5 samples, sequence length 10, 21 features\n",
    "predictions = run_inference(model, test_input)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
