{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundingBox3DNet(\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (dim_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      "  (yaw_conf_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=8, bias=True)\n",
      "  )\n",
      "  (yaw_res_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=16, bias=True)\n",
      "  )\n",
      "  (pitch_conf_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=8, bias=True)\n",
      "  )\n",
      "  (pitch_res_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=16, bias=True)\n",
      "  )\n",
      "  (roll_conf_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=8, bias=True)\n",
      "  )\n",
      "  (roll_res_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=16, bias=True)\n",
      "  )\n",
      "  (trans_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      "  (corner_branch): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=24, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BoundingBox3DNet(nn.Module):\n",
    "    def __init__(self, num_bins=8, feature_extractor='resnet50'):\n",
    "        super(BoundingBox3DNet, self).__init__()\n",
    "        \n",
    "        # Store num_bins\n",
    "        self.num_bins = num_bins\n",
    "        \n",
    "        # Feature extractor (pre-trained backbone)\n",
    "        self.backbone, feature_dim = self.get_backbone(feature_extractor)\n",
    "\n",
    "        # Dimension regression branch\n",
    "        self.dim_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3)  # Output: dx, dy, dz\n",
    "        )\n",
    "\n",
    "        # Yaw orientation (confidence and residuals)\n",
    "        self.yaw_conf_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_bins)\n",
    "        )\n",
    "        self.yaw_res_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2 * num_bins)  # sin(Δθ), cos(Δθ) for each bin\n",
    "        )\n",
    "\n",
    "        # Pitch orientation\n",
    "        self.pitch_conf_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_bins)\n",
    "        )\n",
    "        self.pitch_res_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2 * num_bins)\n",
    "        )\n",
    "\n",
    "        # Roll orientation\n",
    "        self.roll_conf_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_bins)\n",
    "        )\n",
    "        self.roll_res_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2 * num_bins)\n",
    "        )\n",
    "\n",
    "        # Translation regression branch\n",
    "        self.trans_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3)  # Output: tx, ty, tz\n",
    "        )\n",
    "\n",
    "        # Optional: Corner prediction branch\n",
    "        self.corner_branch = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 24)  # Output: 8 corners (normalized positions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "\n",
    "        # Dimensions\n",
    "        dims = self.dim_branch(features)\n",
    "\n",
    "        # Yaw orientation\n",
    "        yaw_conf = self.yaw_conf_branch(features)\n",
    "        yaw_res = self.yaw_res_branch(features)\n",
    "        yaw_res = yaw_res.view(-1, self.num_bins, 2)  # Reshape to (batch, bins, [sin, cos])\n",
    "\n",
    "        # Pitch orientation\n",
    "        pitch_conf = self.pitch_conf_branch(features)\n",
    "        pitch_res = self.pitch_res_branch(features)\n",
    "        pitch_res = pitch_res.view(-1, self.num_bins, 2)\n",
    "\n",
    "        # Roll orientation\n",
    "        roll_conf = self.roll_conf_branch(features)\n",
    "        roll_res = self.roll_res_branch(features)\n",
    "        roll_res = roll_res.view(-1, self.num_bins, 2)\n",
    "\n",
    "        # Translation\n",
    "        translation = self.trans_branch(features)\n",
    "\n",
    "        # Corners (optional)\n",
    "        corners = self.corner_branch(features)\n",
    "\n",
    "        return dims, yaw_conf, yaw_res, pitch_conf, pitch_res, roll_conf, roll_res, translation, corners\n",
    "\n",
    "    def get_backbone(self, model_name):\n",
    "        if model_name == 'resnet50':\n",
    "            from torchvision.models import resnet50\n",
    "            backbone = resnet50(pretrained=True)\n",
    "            # Remove the fully connected layer\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "            feature_dim = 2048\n",
    "        elif model_name == 'vgg16':\n",
    "            from torchvision.models import vgg16\n",
    "            backbone = vgg16(pretrained=True)\n",
    "            backbone = nn.Sequential(*list(backbone.features.children()))\n",
    "            feature_dim = 512\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        return backbone, feature_dim\n",
    "\n",
    "# Instantiate the model\n",
    "model = BoundingBox3DNet(num_bins=8)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 matching files.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class BoundingBox3DDataset(Dataset):\n",
    "    def __init__(self, image_dir, data_2d_dir, data_3d_dir, camera_intrinsics, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.data_2d_dir = data_2d_dir\n",
    "        self.data_3d_dir = data_3d_dir\n",
    "        self.camera_intrinsics = camera_intrinsics\n",
    "        self.transform = transform\n",
    "\n",
    "        # Match files across all directories\n",
    "        self.file_indices = [\n",
    "            os.path.splitext(f)[0]\n",
    "            for f in os.listdir(self.image_dir)\n",
    "            if f.endswith(\".png\") and\n",
    "            os.path.exists(os.path.join(self.data_2d_dir, f\"{os.path.splitext(f)[0]}.csv\")) and\n",
    "            os.path.exists(os.path.join(self.data_3d_dir, f\"{os.path.splitext(f)[0]}.csv\"))\n",
    "        ]\n",
    "        print(f\"Found {len(self.file_indices)} matching files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self._get_num_objects(file_index) for file_index in self.file_indices)\n",
    "\n",
    "    def _get_num_objects(self, file_index):\n",
    "        \"\"\"Helper function to count the number of objects in a file.\"\"\"\n",
    "        data_3d_path = os.path.join(self.data_3d_dir, f\"{file_index}.csv\")\n",
    "        data_3d = pd.read_csv(data_3d_path, header=None)\n",
    "        return len(data_3d)\n",
    "\n",
    "    def __getitem__(self, global_idx):\n",
    "        # Map global index to file index and object index within that file\n",
    "        current_idx = global_idx\n",
    "        for file_index in self.file_indices:\n",
    "            num_objects = self._get_num_objects(file_index)\n",
    "            if current_idx < num_objects:\n",
    "                object_idx = current_idx\n",
    "                break\n",
    "            current_idx -= num_objects\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_dir, f\"{file_index}.png\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load 2D data\n",
    "        data_2d_path = os.path.join(self.data_2d_dir, f\"{file_index}.csv\")\n",
    "        data_2d = pd.read_csv(data_2d_path, header=None).iloc[object_idx].values.flatten()\n",
    "        bb_center = torch.tensor(data_2d[1:3], dtype=torch.float32)\n",
    "        bb_size = torch.tensor(data_2d[3:5], dtype=torch.float32)\n",
    "        corners_2d = torch.tensor(data_2d[5:], dtype=torch.float32)\n",
    "\n",
    "        # Load 3D data\n",
    "        data_3d_path = os.path.join(self.data_3d_dir, f\"{file_index}.csv\")\n",
    "        data_3d = pd.read_csv(data_3d_path, header=None).iloc[object_idx].values.flatten()\n",
    "\n",
    "        # Validate 3D corner data\n",
    "        if len(data_3d[7:]) != 24:\n",
    "            raise ValueError(f\"Skipping invalid object in file {file_index}: Expected 24 corner values, got {len(data_3d[7:])}\")\n",
    "\n",
    "        corners_3d = torch.tensor(data_3d[7:], dtype=torch.float32).reshape(8, 3)\n",
    "\n",
    "        # Calculate dimensions (dx, dy, dz)\n",
    "        dims = torch.max(corners_3d, dim=0).values - torch.min(corners_3d, dim=0).values\n",
    "\n",
    "        # Camera intrinsics\n",
    "        camera_intrinsics = torch.tensor(self.camera_intrinsics, dtype=torch.float32)\n",
    "\n",
    "        sample = {\n",
    "            \"image\": image,\n",
    "            \"bb_center\": bb_center,\n",
    "            \"bb_size\": bb_size,\n",
    "            \"corners_2d\": corners_2d,\n",
    "            \"position\": torch.tensor(data_3d[1:4], dtype=torch.float32),\n",
    "            \"rotation\": torch.tensor(data_3d[4:7], dtype=torch.float32),\n",
    "            \"corners_3d\": corners_3d,\n",
    "            \"dims\": dims,\n",
    "            \"camera_intrinsics\": camera_intrinsics,\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "# Camera intrinsics matrix\n",
    "camera_intrinsics = np.array([\n",
    "    [1108.513, 0, 640, 0],\n",
    "    [0, 623.5383, 360, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# Paths to directories\n",
    "image_dir = r\"C:\\Users\\sakar\\OneDrive\\mt-datas\\synthetic_data\\8_correct_relative\\images\\train\"\n",
    "data_2d_dir = r\"C:\\Users\\sakar\\OneDrive\\mt-datas\\synthetic_data\\8_correct_relative\\2d_data\"\n",
    "data_3d_dir = r\"C:\\Users\\sakar\\OneDrive\\mt-datas\\synthetic_data\\8_correct_relative\\3d_data\"\n",
    "\n",
    "# Define transformations for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "dataset = BoundingBox3DDataset(image_dir, data_2d_dir, data_3d_dir, camera_intrinsics, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Start training with early stopping and model saving\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[43mtrain_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    171\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 68\u001b[0m, in \u001b[0;36mtrain_with_early_stopping\u001b[1;34m(model, train_loader, val_loader, optimizer, num_epochs, device, patience, save_dir)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Ensure the save directory exists\u001b[39;00m\n\u001b[0;32m     66\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 68\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39mpatience)\n\u001b[0;32m     70\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sakar\\anaconda3\\envs\\atlantanet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Loss functions\n",
    "def multibin_loss(conf, residuals, gt_orientation, num_bins=8):\n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "    bin_indices = (gt_orientation // bin_width).long()\n",
    "    bin_indices = torch.clamp(bin_indices, min=0, max=conf.size(1) - 1)  # Ensure indices are valid\n",
    "\n",
    "    # Confidence loss\n",
    "    conf_loss = F.cross_entropy(conf, bin_indices)\n",
    "\n",
    "    # Residual loss\n",
    "    delta = gt_orientation - bin_indices * bin_width\n",
    "    sin_gt, cos_gt = torch.sin(delta), torch.cos(delta)\n",
    "    residual_loss = F.mse_loss(residuals[:, :, 0], sin_gt) + F.mse_loss(residuals[:, :, 1], cos_gt)\n",
    "\n",
    "    return conf_loss + residual_loss\n",
    "\n",
    "def dimension_loss(pred_dims, gt_dims):\n",
    "    return F.mse_loss(pred_dims, gt_dims)\n",
    "\n",
    "def translation_loss(pred_trans, gt_trans):\n",
    "    return F.mse_loss(pred_trans, gt_trans)\n",
    "\n",
    "# Early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_with_early_stopping(\n",
    "    model, train_loader, val_loader, optimizer, num_epochs=20, device=\"cuda\", patience=5, save_dir=\"./models\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and save the best model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        optimizer: Optimizer for training.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        device: Device to train on ('cuda' or 'cpu').\n",
    "        patience: Number of epochs to wait before stopping if no improvement.\n",
    "        save_dir: Directory to save the model weights.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    model = model.to(device)\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    best_model = None\n",
    "    best_epoch = -1\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move data to device\n",
    "            images = batch[\"image\"].to(device)\n",
    "            gt_dims = batch[\"dims\"].to(device)  # Use 3D dimensions (dx, dy, dz)\n",
    "            gt_orientations = batch[\"rotation\"].to(device)\n",
    "            gt_positions = batch[\"position\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_dims, yaw_conf, yaw_res, pitch_conf, pitch_res, roll_conf, roll_res, pred_trans, _ = model(images)\n",
    "\n",
    "            # Extract individual ground-truth orientation values\n",
    "            gt_yaw = gt_orientations[:, 0]  # Yaw\n",
    "            gt_pitch = gt_orientations[:, 1]  # Pitch\n",
    "            gt_roll = gt_orientations[:, 2]  # Roll\n",
    "\n",
    "            # Loss calculation\n",
    "            loss_dim = dimension_loss(pred_dims, gt_dims)\n",
    "            loss_yaw = multibin_loss(yaw_conf, yaw_res, gt_yaw)\n",
    "            loss_pitch = multibin_loss(pitch_conf, pitch_res, gt_pitch)\n",
    "            loss_roll = multibin_loss(roll_conf, roll_res, gt_roll)\n",
    "            loss_orient = loss_yaw + loss_pitch + loss_roll\n",
    "            loss_trans = translation_loss(pred_trans, gt_positions)\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss_dim + loss_orient + loss_trans\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch[\"image\"].to(device)\n",
    "                gt_dims = batch[\"dims\"].to(device)  # Use 3D dimensions (dx, dy, dz)\n",
    "                gt_orientations = batch[\"rotation\"].to(device)\n",
    "                gt_positions = batch[\"position\"].to(device)\n",
    "\n",
    "                pred_dims, yaw_conf, yaw_res, pitch_conf, pitch_res, roll_conf, roll_res, pred_trans, _ = model(images)\n",
    "\n",
    "                gt_yaw = gt_orientations[:, 0]\n",
    "                gt_pitch = gt_orientations[:, 1]\n",
    "                gt_roll = gt_orientations[:, 2]\n",
    "\n",
    "                loss_dim = dimension_loss(pred_dims, gt_dims)\n",
    "                loss_yaw = multibin_loss(yaw_conf, yaw_res, gt_yaw)\n",
    "                loss_pitch = multibin_loss(pitch_conf, pitch_res, gt_pitch)\n",
    "                loss_roll = multibin_loss(roll_conf, roll_res, gt_roll)\n",
    "                loss_orient = loss_yaw + loss_pitch + loss_roll\n",
    "                loss_trans = translation_loss(pred_trans, gt_positions)\n",
    "\n",
    "                val_loss += (loss_dim + loss_orient + loss_trans).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = model.state_dict()\n",
    "            model_save_path = os.path.join(save_dir, f\"best_model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(best_model, model_save_path)\n",
    "            print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Final save of the best model\n",
    "    final_model_path = os.path.join(save_dir, \"best_model_final.pth\")\n",
    "    if best_model:\n",
    "        torch.save(best_model, final_model_path)\n",
    "        print(f\"Final best model saved at {final_model_path}\")\n",
    "    else:\n",
    "        print(\"No best model was saved.\")\n",
    "\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = BoundingBox3DNet(num_bins=8)  # Ensure this aligns with the model definition\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Start training with early stopping and model saving\n",
    "train_with_early_stopping(\n",
    "    model, train_loader, val_loader, optimizer, num_epochs=20, patience=5, save_dir=\"./models\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlantanet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
